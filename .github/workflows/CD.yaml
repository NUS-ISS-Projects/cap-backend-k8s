name: Deploy DIS Platform to GKE

on:
  push:
    tags:
      - '*-release' # e.g., v1.0-release
      - '*-staging' # e.g., v1.1-staging

env:
  PROJECT_ID: ${{ secrets.PROJECT_ID }}
  GKE_CLUSTER: ${{ secrets.GKE_CLUSTER }}
  GKE_ZONE: ${{ secrets.GKE_ZONE }}
  GHCR_USERNAME: ${{ secrets.GHCR_USERNAME }}
  GHCR_TOKEN: ${{ secrets.GHCR_TOKEN }}
  GCLOUD_AUTH: ${{ secrets.GCLOUD_AUTH }}

jobs:
  deploy-dis-platform-to-gke:
    name: Deploy DIS Platform to GKE
    runs-on: ubuntu-latest
    steps:
      # Step 1: Checkout code
      - name: Checkout code
        uses: actions/checkout@v2

      # Step 2: Authenticate to GCP
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v2
        with:
          credentials_json: '${{ env.GCLOUD_AUTH }}'

      # Step 3: Set up Cloud SDK
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2

      # Step 4: Install the GKE gcloud auth plugin
      - name: Install gke-gcloud-auth-plugin
        run: |
          gcloud components install gke-gcloud-auth-plugin --quiet

      # Step 5: Enable the GKE Auth Plugin for kubectl
      - name: Configure GKE Auth Plugin
        run: echo "export USE_GKE_GCLOUD_AUTH_PLUGIN=True" >> $GITHUB_ENV

      # Step 6: Get GKE credentials
      - name: Get GKE credentials
        run: |
          gcloud container clusters get-credentials ${{ env.GKE_CLUSTER }} --zone ${{ env.GKE_ZONE }} --project ${{ env.PROJECT_ID }}

      # Step 7: Create the GHCR Docker config file for use with Kustomize secretGenerator
      # This secret ('ghcr-secret') is referenced in your k8s deployment YAMLs for imagePullSecrets
      - name: Create GHCR Docker config for Kustomize
        run: |
          mkdir -p k8s/base # Ensure the directory exists
          echo '{"auths":{"ghcr.io":{"auth":"'"$(echo -n "${{ env.GHCR_USERNAME }}:${{ env.GHCR_TOKEN }}" | base64 -w0)"'"}}}' > k8s/base/.dockerconfigjson
        # The '-w0' for base64 is important to avoid line wraps.

      # Step 8: Install Kustomize
      - name: Install Kustomize
        run: |
          curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh" | bash
          sudo mv kustomize /usr/local/bin/

      # Step 9: Apply Kustomize for production
      - name: Apply Kubernetes Manifests for production
        if: contains(github.ref, '-release')
        run: |
          kustomize build ./k8s/overlays/prod | kubectl apply -f -

      # Step 10: Apply Kustomize for staging (if you have a staging overlay)
      - name: Apply Kubernetes Manifests for staging
        if: contains(github.ref, '-staging')
        run: |
          kustomize build ./k8s/overlays/staging | kubectl apply -f -

      # Step 11: Check the deployment status for DIS platform services
      - name: Check Rollout Status for DIS Platform
        id: rollout_status
        run: |
          deployments=("data-ingestion-service" "data-processing-service" "data-acquisition-service" "postgres")
          statefulsets=("kafka" "zookeeper")
          failed_components_list="" # Comma-separated list for output

          for dep_name in "${deployments[@]}"; do
            echo "Checking rollout status for deployment $dep_name..."
            if ! kubectl rollout status deployment/$dep_name --namespace default --timeout=5m; then
              echo "Rollout failed for deployment/$dep_name"
              failed_components_list="${failed_components_list}deployment/$dep_name,"
            fi
          done

          for sts_name in "${statefulsets[@]}"; do
            echo "Checking rollout status for statefulset $sts_name..."
            # For StatefulSets, a successful update means all pods are running and ready with the new revision.
            # `kubectl rollout status statefulset` is the correct command.
            if ! kubectl rollout status statefulset/$sts_name --namespace default --timeout=5m; then
              echo "Rollout failed for statefulset/$sts_name"
              failed_components_list="${failed_components_list}statefulset/$sts_name,"
            fi
          done

          if [ -n "$failed_components_list" ]; then
            # Remove trailing comma
            failed_components_list=${failed_components_list%,}
            echo "::set-output name=failed_components::$failed_components_list"
            echo "Deployment failed for the following components: $failed_components_list"
            exit 1 # Make the job fail
          else
            echo "All DIS platform components deployed successfully!"
            echo "::set-output name=failed_components::" # Empty string if success
          fi

      # Step 12: Rollback deployment if any service fails
      # This is a simplified rollback; StatefulSet rollbacks can be more complex.
      - name: Rollback Failed Components
        if: failure() && steps.rollout_status.outputs.failed_components != ''
        run: |
          IFS=',' read -r -a components_to_rollback <<< "${{ steps.rollout_status.outputs.failed_components }}"
          for component in "${components_to_rollback[@]}"; do
            echo "Attempting to rollback $component..."
            if [[ $component == deployment/* ]]; then
              kubectl rollout undo $component --namespace default
              echo "Rolled back $component."
            elif [[ $component == statefulset/* ]]; then
              kubectl rollout undo $component --namespace default
              # Note: StatefulSet undo might have limitations or require manual intervention for data consistency
              echo "Attempted rollback for $component. Review its status manually."
            fi
          done
          echo "Rollback process attempted for failed components."

      # Step 13: Clean up
      - name: Clean up temporary files
        if: always() # Ensure cleanup runs even if previous steps fail
        run: |
          rm -f k8s/base/.dockerconfigjson
          echo "Cleaned up temporary files."


  dast-baseline-scan:
    name: DAST Baseline Scan (OWASP ZAP)
    runs-on: ubuntu-latest
    # This job needs the deployment job to complete successfully
    needs: deploy-dis-platform-to-gke
    # Only run this job if a staging tag was pushed AND deployment was successful
    if: success() && contains(github.ref, '-staging') && needs.deploy-dis-platform-to-gke.outputs.deployment_status == 'success'

    steps:
      - name: Checkout code
        uses: actions/checkout@v3
        with:
          # We need the ZAP configuration file if you add one, or for context
          fetch-depth: 0 

      - name: OWASP ZAP Baseline Scan
        uses: zaproxy/action-baseline@v0.11.0 # Or the latest version
        with:
          # Target URL of your STAGING application
          target: ${{ env.STAGING_APP_URL }}
          # Docker image name (default is owasp/zap2docker-stable)
          # docker_name: 'owasp/zap2docker-stable'
          # Rules file (optional, for more customized passive scanning rules)
          # rules_file_name: 'zap-baseline-rules.tsv' 
          # Generate report in HTML and Markdown format
          cmd_options: '-J report.json -r report.html -x report.xml'
          # Optionally, fail the build if certain alert levels are found
          # fail_action: true # This would fail the action if any alerts are found by default
          # GITHUB_TOKEN is automatically provided by GitHub Actions
          # token: ${{ secrets.GITHUB_TOKEN }} # Only needed for issue creation, not baseline scan report artifact

      - name: Upload ZAP Scan Report
        if: always() # Ensure report is uploaded even if ZAP finds issues (and doesn't fail the action)
        uses: actions/upload-artifact@v3
        with:
          name: zap-baseline-scan-report
          path: |
            report.html
            report.json
            report.xml