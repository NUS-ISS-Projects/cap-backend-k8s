4.2 Availability
Availability refers to the system's ability to remain operational and accessible over time, minimizing downtime and ensuring resilience to failures.
Mechanisms and Design Choices for Availability:
Redundancy and Fault Tolerance (Kubernetes-Native):
Pod Replication & Self-Healing: The application services (data-ingestion, data-processing, data-acquisition) are deployed as Kubernetes Deployments. Kubernetes automatically maintains the desired number of pod replicas. If a pod or the node it's running on fails, Kubernetes will reschedule and restart the pod on a healthy node.
Horizontal Pod Autoscaling (HPA): HPAs not only manage performance but also contribute to availability by ensuring a minReplicas count (currently 1) is always running and can scale up to maxReplicas (currently 2) to handle load or transient issues affecting a single pod.
Health Probes: All application Deployments are configured with Liveness and Readiness probes that target their respective /health API endpoints.
Readiness Probes (readinessProbe): Ensure that network traffic is only routed to pods that have fully started and are ready to serve requests.
Liveness Probes (livenessProbe): Allow Kubernetes to detect and automatically restart unresponsive or deadlocked application instances.
StatefulSet for Backend Services: Kafka and Zookeeper are deployed as StatefulSets. While currently single-replica, StatefulSets are designed for stateful applications, providing stable network identifiers and ordered, graceful deployment and scaling, which is important for these distributed systems.

Figure 4.2.a: HPA services

Figure 4.2.b: Probes for healthiness check
Application Fallbacks: The RealTimeMetricsService includes a try-catch block and a fallback mechanism. If it fails to connect to the data-ingestion-service for real-time metrics, it logs the error and returns a default RealTimeMetrics object instead of failing the request, improving the resilience of the /api/acquisition/realtime endpoint.

Data Pipeline Resilience:
Kafka as a Durable Buffer: Apache Kafka acts as a persistent message queue between the data-ingestion-service and the data-processing-service. If the data-processing-service instances become temporarily unavailable, PDUs published by the data-ingestion-service are retained in Kafka topics. Once the processing service recovers, it can resume consuming messages from where it left off (based on consumer group offsets), preventing data loss during transient outages.
Fallback Mechanisms: The RealTimeMetricsService within data-acquisition-service implements error handling when fetching metrics from data-ingestion-service. If the remote call fails (e.g., due to a network issue or data-ingestion-service being down), it logs the error and returns a default/empty RealTimeMetrics object instead of failing the entire request, ensuring the /api/acquisition/realtime endpoint remains partially available.

Zero-Downtime Deployments:
Rolling Updates: The Kubernetes Deployments for application services are configured with the RollingUpdate strategy (strategy: type: RollingUpdate). This ensures that when a new version of a service is deployed (e.g., via the CI/CD pipeline), Kubernetes updates pods incrementally—creating new pods and terminating old ones gradually—thus maintaining service availability during the update process.

Figure 4.2.c: Rolling updates
CI/CD Rollout Verification: The GitHub Actions CD pipeline includes a "Check Rollout Status for DIS Platform" step that uses kubectl rollout status to verify that deployments and statefulsets have successfully completed their updates and new pods are ready before marking the deployment as successful. If a rollout fails, a subsequent step attempts an automated rollback.

Figure 4.2.d: Rolling update status
		
Potential Availability Enhancements for Production:
Persistent Storage for Stateful Services:
PostgreSQL: The current PostgreSQL StatefulSet is configured with a PersistentVolumeClaim (PVC) using 20Gi of durable storage with the standard-rwo storage class, ensuring data persistence across pod restarts and rescheduling. This provides a solid foundation for production use, though additional high-availability configurations may be considered for enhanced resilience.
Kafka & Zookeeper: The Kafka and Zookeeper StatefulSets should be configured with PVCs for their data directories to ensure data persistence across pod restarts and rescheduling in production environments.
High Availability for Backend Services:
PostgreSQL: Implement a high-availability PostgreSQL setup, such as using a managed cloud database service (e.g., Cloud SQL with HA configuration) or deploying a PostgreSQL cluster with replication (e.g., using Patroni).
Kafka & Zookeeper: Increase the replica count for Kafka brokers and Zookeeper instances (e.g., to 3 or 5 replicas) and configure them for clustered operation to tolerate node failures. This includes setting appropriate replication factors for Kafka topics.
Increased Replica Counts: For stateless application services, increase the minReplicas in HPAs and potentially the default replicas in Deployments (e.g., to 2 or 3) for production environments to ensure at least two instances are always running, providing immediate failover capability.
Cross-Zone/Region Deployments (Advanced): For very high availability requirements, consider deploying the GKE cluster and critical services across multiple zones or even regions, along with global load balancing.
